{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What?\n",
    "* visualize your Tensorflow graph\n",
    "* plot quantative metrics\n",
    "* show data\n",
    "\n",
    "Why?\n",
    "* easier to understand\n",
    "* easier to debug\n",
    "* easier to optimize\n",
    "\n",
    "* Most higher level libraries support it and make using it very effortless:\n",
    "    * TFLeanr\n",
    "    * Keras\n",
    "    * ...\n",
    "    * Not so much with Edward :/\n",
    "\n",
    "\n",
    "* Actively developed, more coming:\n",
    "    * TensorFlow Debugger integration (See if/where there were nans etc)\n",
    "    * Plugins (people contributing domain specific visualizations)\n",
    "    * Scalabale TensorBoard (share withing research group)\n",
    "    \n",
    "Why not?\n",
    "* Not so much documentation\n",
    "* Not many examples\n",
    "* Almost all examples contributed by other people do not work with current version\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [summary operations](https://www.tensorflow.org/versions/r0.11/api_docs/python/train/#summary_operations) to collect summary data from graph nodes. Summary operations are also given a tag / name that will be used to organize the data in the frontend.\n",
    ">tf.summary.tensor_summary <br>\n",
    ">tf.summary.scalar <br>\n",
    ">tf.summary.histogram <br>\n",
    ">tf.summary.audio <br>\n",
    ">tf.summary.image\n",
    "\n",
    "Then merge all your summary operations in to one:\n",
    "> tf.summary.merge_all \n",
    "\n",
    "Write all summary data to disk using FileWriter. \n",
    "> file_writer = tf.summary.FileWriter('/path/to/logs', sess.graph)\n",
    "\n",
    "When running tensorboard, it recursively walks the directory given to it looking for subdirectories that contain tfevents data. It will visualize all these thus making comparison between different runs easier. You can even give several directories separated with commas.\n",
    "Run tensorboard:\n",
    "> tensorboard --logdir=path/to/log-directory --port=6006\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## MNIST Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing tensorflow graph: simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simple model for classifying mnist data\n",
    "# Adapted from: http://ischlag.github.io/2016/06/04/how-to-use-tensorboard/\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "# reset everything to rerun in jupyter\n",
    "tf.reset_default_graph()\n",
    "\n",
    "batch_size = 100\n",
    "learning_rate = 0.5\n",
    "training_epochs = 20\n",
    "logs_path = \"tblogs/reg/\"\n",
    "\n",
    "\n",
    "# Placeholders for inputs\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784]) \n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "# Parameters\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    #\n",
    "    # Write graph using FileWriter\n",
    "    #\n",
    "    writer = tf.summary.FileWriter(logs_path)\n",
    "    writer.add_graph(sess.graph)\n",
    "        \n",
    "    for epoch in range(training_epochs):\n",
    "        batch_count = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(batch_count):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            train = sess.run(train_op, feed_dict={x: batch_x, y_: batch_y})\n",
    "\n",
    "        if epoch % 5 == 0: \n",
    "            print(\"Epoch: \", epoch)\n",
    "    print(\"Accuracy: \", accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding name scopes and summary operations\n",
    "\n",
    "* Adding name scopes helps understanding the graph. \n",
    "    * Graphs can quickly get complicated with thousands of nodes that can not be visualized at once. Name scopes can be used to define a hierarchy on the nodes which tensorboard then utilizes in the visualization.\n",
    "* Adding summaries to visualize input data and results. \n",
    "    * To visualize the summary data in TensorBoard, you should:\n",
    "        * evaluate the summary op\n",
    "        * retrieve the result\n",
    "        * write that result to disk using a summary.FileWriter\n",
    "    * Give tags to organise the summary data in the frontend. The scalar and histogram dashboards organize data by tag, and group the tags into folders according to a directory/like/hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simple model for classifying mnist data\n",
    "# Adapted from: http://ischlag.github.io/2016/06/04/how-to-use-tensorboard/\n",
    "# and https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py\n",
    "\n",
    "\n",
    "# reset everything to rerun in jupyter\n",
    "tf.reset_default_graph()\n",
    "\n",
    "batch_size = 100\n",
    "learning_rate = 0.5\n",
    "training_epochs = 20\n",
    "logs_path = \"tblogs/reg_names/\"\n",
    "\n",
    "#\n",
    "# Add name scopes to define hierarchy on the nodes\n",
    "#\n",
    "\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x-input\")\n",
    "    image_shaped_input = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    tf.summary.image('input', image_shaped_input, 10)\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, 10], name=\"y-input\")\n",
    "\n",
    "with tf.name_scope(\"weights\"):\n",
    "    W = tf.Variable(tf.truncated_normal([784, 10], stddev=0.1))\n",
    "    #W = tf.Variable(tf.zeros([784, 10]))\n",
    "\n",
    "with tf.name_scope(\"biases\"):\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[10]))\n",
    "    #b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "with tf.name_scope(\"softmax\"):\n",
    "    y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "\n",
    "with tf.name_scope('cross_entropy'):\n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#    \n",
    "# Create a summaries and merge them\n",
    "#\n",
    "tf.summary.scalar(\"cost\", cross_entropy)\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "tf.summary.histogram(\"weights\", W)\n",
    "tf.summary.histogram(\"biases\", b)\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    writer = tf.summary.FileWriter(logs_path)\n",
    "    writer.add_graph(sess.graph)\n",
    "        \n",
    "    for epoch in range(training_epochs):\n",
    "        batch_count = int(mnist.train.num_examples/batch_size)\n",
    "        \n",
    "        for i in range(batch_count):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            _, summary = sess.run([train_op, summary_op], feed_dict={x: batch_x, y_: batch_y})       \n",
    "            # write summary log\n",
    "            writer.add_summary(summary, epoch * batch_count + i)\n",
    "            \n",
    "        if epoch % 5 == 0: \n",
    "            print(\"Epoch: \", epoch)\n",
    "            \n",
    "    print(\"Accuracy: \", accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional neural networks made easy with tflearn\n",
    "\n",
    "TFLearn automatically supports TensorBoard. You can control what summaries you need with the verbose argument for DNN model class:\n",
    ">model = DNN(network, tensorboard_verbose=3, tensorboard_dir=logs_path)\n",
    "\n",
    "where the options are:\n",
    "* 0: Loss & Metric (Best speed).\n",
    "* 1: Loss, Metric & Gradients.\n",
    "* 2: Loss, Metric, Gradients & Weights.\n",
    "* 3: Loss, Metric, Gradients, Weights, Activations & Sparsity (Best Visualization).\n",
    "\n",
    "\n",
    "### Comparing different executions of the model\n",
    "\n",
    "* Changing hyperparameters and comparing results visually\n",
    "* Tensorboard recursively walks the directory tree rooted at logdir looking for subdirectories that contain tfevents data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/tflearn/tflearn/blob/master/examples/images/convnet_mnist.py\n",
    "\n",
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from tflearn.layers.estimator import regression\n",
    "\n",
    "\n",
    "def make_modelid(learning_rate, two_conv_layers, two_fc_layers):\n",
    "    return 'cnn_lr:{0}_conv:{1}_fc:{2}'.format(learning_rate, two_conv_layers, two_fc_layers)\n",
    "\n",
    "# Data loading and preprocessing\n",
    "import tflearn.datasets.mnist as mnist\n",
    "X, Y, testX, testY = mnist.load_data(one_hot=True)\n",
    "X = X.reshape([-1, 28, 28, 1])\n",
    "testX = testX.reshape([-1, 28, 28, 1])\n",
    "\n",
    "batch_size = 100\n",
    "training_epochs = 5\n",
    "logs_path = \"tblogs_multiple\"\n",
    "\n",
    "for learning_rate in [0.1, 0.01, 0.001]:\n",
    "    for two_conv_layers in [True, False]:\n",
    "        for two_fc_layers in [True, False]:\n",
    "            tf.reset_default_graph()\n",
    "\n",
    "            network = input_data(shape=[None, 28, 28, 1], name='input')\n",
    "            network = conv_2d(network, 32, 3, activation='relu', regularizer=\"L2\")\n",
    "            network = max_pool_2d(network, 2)\n",
    "            if (two_conv_layers):\n",
    "                network = conv_2d(network, 64, 3, activation='relu', regularizer=\"L2\")\n",
    "                network = max_pool_2d(network, 2)\n",
    "\n",
    "            network = fully_connected(network, 128, activation='tanh')\n",
    "            if (two_fc_layers):\n",
    "                network = fully_connected(network, 256, activation='tanh')\n",
    "                \n",
    "            network = fully_connected(network, 10, activation='softmax')\n",
    "            network = regression(network, optimizer='adam', learning_rate=learning_rate,\n",
    "                                 loss='categorical_crossentropy', name='target')\n",
    "\n",
    "            # Training\n",
    "            model = tflearn.DNN(network, tensorboard_verbose=2, tensorboard_dir=logs_path)\n",
    "            model_id = make_modelid(learning_rate, two_conv_layers, two_fc_layers)\n",
    "            model.fit(X, Y, n_epoch=training_epochs, shuffle=True, show_metric=True, batch_size=batch_size,\n",
    "                        snapshot_epoch=True, run_id=model_id,\n",
    "                        validation_set=(testX, testY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edward and TensorBoard\n",
    "* Visualizing graph possible. Initialize takes log directory as a parameter.\n",
    "\n",
    ">initialize(logdir='tblogs')\n",
    "\n",
    "* [Summaries](https://github.com/blei-lab/edward/issues/478) don't really work, but tensorboard will be used for more visualizations [later](https://github.com/blei-lab/edward/issues/190) (?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copied from GitHub examples: edward/examples/normal_normal_tensorboard.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import edward as ed\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from edward.models import Normal\n",
    "\n",
    "ed.set_seed(42)\n",
    "\n",
    "# DATA\n",
    "x_data = np.array([0.0] * 50)\n",
    "\n",
    "# MODEL: Normal-Normal with known variance\n",
    "mu = Normal(loc=0.0, scale=1.0, name='mu')\n",
    "x = Normal(loc=tf.ones(50) * mu, scale=1.0, name='x')\n",
    "\n",
    "# INFERENCE\n",
    "qmu_mu = tf.Variable(tf.random_normal([]), name='qmu_mu')\n",
    "qmu_sigma = tf.nn.softplus(tf.Variable(tf.random_normal([])), name='qmu_sigma')\n",
    "qmu = Normal(loc=qmu_mu, scale=qmu_sigma, name='qmu')\n",
    "\n",
    "# analytic solution: N(loc=0.0, scale=\\sqrt{1/51}=0.140)\n",
    "inference = ed.KLqp({mu: qmu}, data={x: x_data})\n",
    "inference.run(logdir='tblogs_edward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Visualization\n",
    "* Interactive visualization and analysis of high-dimensional data\n",
    "* Two- or three-dimensional view using\n",
    "    * PCA: The Embedding Projector computes the top 10 principal components. The menu lets you project those components onto any combination of two or three.\n",
    "    * t-SNE: Layout is performed client-side animating every step of the algorithm. \n",
    "    * Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example from https://github.com/anujshah1003/Tensorboard-examples/blob/master/mnist-tensorboard-embeddings-1.py\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import numpy as np\n",
    "\n",
    "PATH = os.getcwd()\n",
    "\n",
    "LOG_DIR = PATH + '/mnist-tensorboard/log-1'\n",
    "metadata = os.path.join(LOG_DIR, 'metadata.tsv')\n",
    "mnist = input_data.read_data_sets(PATH + \"/mnist-tensorboard/data/\", one_hot=True)\n",
    "\n",
    "# Setup a 2D tensor that holds your embedding(s)\n",
    "images = tf.Variable(mnist.test.images, name='images')\n",
    "\n",
    "# Create metadata\n",
    "with open(metadata, 'w') as metadata_file:\n",
    "    for row in range(10000):\n",
    "        c = np.nonzero(mnist.test.labels[::1])[1:][0][row]\n",
    "        metadata_file.write('{}\\n'.format(c))\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Save your model variables in a checkpoint in LOG_DIR.\n",
    "    saver = tf.train.Saver([images])\n",
    "    sess.run(images.initializer)\n",
    "    saver.save(sess, os.path.join(LOG_DIR, 'images.ckpt'))\n",
    "\n",
    "    config = projector.ProjectorConfig()\n",
    "    # Add embedding(s)\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = images.name\n",
    "    # Link to metadata\n",
    "    embedding.metadata_path = metadata\n",
    "    # Write a projector_config.pbtxt in the LOG_DIR. TensorBoard will\n",
    "    # read this file during startup.\n",
    "    projector.visualize_embeddings(tf.summary.FileWriter(LOG_DIR), config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
